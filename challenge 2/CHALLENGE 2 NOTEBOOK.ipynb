{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"none","dataSources":[{"sourceId":11945612,"sourceType":"datasetVersion","datasetId":7509714}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport cv2\nimport torch\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom sklearn.svm import OneClassSVM\nfrom sklearn.metrics import classification_report\n\nfrom torchvision import models, transforms\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn as nn\n","metadata":{"id":"jFDEHNDxojp3","trusted":true,"execution":{"iopub.status.busy":"2025-05-25T14:40:41.377319Z","iopub.execute_input":"2025-05-25T14:40:41.377650Z","iopub.status.idle":"2025-05-25T14:40:41.382877Z","shell.execute_reply.started":"2025-05-25T14:40:41.377626Z","shell.execute_reply":"2025-05-25T14:40:41.382125Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"train_image_dir = \"/kaggle/input/binary-classification/soil_competition-2025/train\"      # All soil images\ntest_image_dir = \"/kaggle/input/binary-classification/soil_competition-2025/test\"        # Soil and non-soil images\ntest_id_file = \"/kaggle/input/binary-classification/soil_competition-2025/test_ids.csv\" # Or .txt or other, update if needed\n","metadata":{"id":"w7YOUQMMpgTS","trusted":true,"execution":{"iopub.status.busy":"2025-05-25T14:41:08.832753Z","iopub.execute_input":"2025-05-25T14:41:08.833065Z","iopub.status.idle":"2025-05-25T14:41:08.837657Z","shell.execute_reply.started":"2025-05-25T14:41:08.833047Z","shell.execute_reply":"2025-05-25T14:41:08.836612Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"import pandas as pd\n\n# Create dataframe for training: All images are soil => label = 1\ntrain_files = [f for f in os.listdir(train_image_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\ntrain_df = pd.DataFrame({'image_id': train_files})\ntrain_df['label'] = 1\n\n# Remove anything that's not a real image\ntest_files = [f for f in os.listdir(test_image_dir)\n              if f.lower().endswith(('.png', '.jpg', '.jpeg')) and f != 'image_id']\ntest_df = pd.DataFrame({'image_id':test_files})","metadata":{"id":"X1I49FKBpzC_","trusted":true,"execution":{"iopub.status.busy":"2025-05-25T14:41:13.473374Z","iopub.execute_input":"2025-05-25T14:41:13.473732Z","iopub.status.idle":"2025-05-25T14:41:13.487284Z","shell.execute_reply.started":"2025-05-25T14:41:13.473708Z","shell.execute_reply":"2025-05-25T14:41:13.486466Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"test_df","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"id":"qILbAuaOp6Rx","outputId":"3e44bfb4-d844-4da2-9214-c15c56bdab6f","trusted":true,"execution":{"iopub.status.busy":"2025-05-25T14:41:17.134432Z","iopub.execute_input":"2025-05-25T14:41:17.134753Z","iopub.status.idle":"2025-05-25T14:41:17.144128Z","shell.execute_reply.started":"2025-05-25T14:41:17.134730Z","shell.execute_reply":"2025-05-25T14:41:17.143256Z"}},"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"                                 image_id\n0    465084323936570da664f0ca8dc90326.jpg\n1    1aa0b12029d35e778dba5bff1255c638.jpg\n2    6df2c3dcd4fb59298c7a73467ea72eeb.jpg\n3    107f25ebd87f581ea57c630a2dcdf50c.jpg\n4    dc35d58782615e4f9582c6b32c8b956e.jpg\n..                                    ...\n962  ef98accfe0ea56499544211d9c96056b.jpg\n963  31475ede49d15c279ef04d048c6f059c.jpg\n964  b954b5ae3f475d399bdec4b036ad0628.jpg\n965  f65a998dafe653e19762b202c0ee5815.jpg\n966  5282098dcede56a9a1424fb6608efa52.jpg\n\n[967 rows x 1 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>465084323936570da664f0ca8dc90326.jpg</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1aa0b12029d35e778dba5bff1255c638.jpg</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>6df2c3dcd4fb59298c7a73467ea72eeb.jpg</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>107f25ebd87f581ea57c630a2dcdf50c.jpg</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>dc35d58782615e4f9582c6b32c8b956e.jpg</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>962</th>\n      <td>ef98accfe0ea56499544211d9c96056b.jpg</td>\n    </tr>\n    <tr>\n      <th>963</th>\n      <td>31475ede49d15c279ef04d048c6f059c.jpg</td>\n    </tr>\n    <tr>\n      <th>964</th>\n      <td>b954b5ae3f475d399bdec4b036ad0628.jpg</td>\n    </tr>\n    <tr>\n      <th>965</th>\n      <td>f65a998dafe653e19762b202c0ee5815.jpg</td>\n    </tr>\n    <tr>\n      <th>966</th>\n      <td>5282098dcede56a9a1424fb6608efa52.jpg</td>\n    </tr>\n  </tbody>\n</table>\n<p>967 rows × 1 columns</p>\n</div>"},"metadata":{}}],"execution_count":24},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nimport cv2\nfrom torchvision import transforms\n\nclass SoilDataset(Dataset):\n    def __init__(self, dataframe, image_dir, transform=None, include_label=True):\n        self.dataframe = dataframe\n        self.image_dir = image_dir\n        self.transform = transform\n        self.include_label = include_label\n\n    def __len__(self):\n        return len(self.dataframe)\n\n    def __getitem__(self, idx):\n        row = self.dataframe.iloc[idx]\n        img_path = os.path.join(self.image_dir, row.image_id)\n\n        # Safely load image\n        if not os.path.exists(img_path):\n            raise FileNotFoundError(f\"❌ Image not found: {img_path}\")\n\n        image = cv2.imread(img_path)\n        if image is None:\n            raise ValueError(f\"❌ Could not read image: {img_path}\")\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n        if self.transform:\n            image = self.transform(image)\n\n        if self.include_label:\n            label = row.label\n            return image, label\n        else:\n            return image, row.image_id\n","metadata":{"id":"L1PFfZKLp8PJ","trusted":true,"execution":{"iopub.status.busy":"2025-05-25T14:41:20.834778Z","iopub.execute_input":"2025-05-25T14:41:20.835798Z","iopub.status.idle":"2025-05-25T14:41:20.843505Z","shell.execute_reply.started":"2025-05-25T14:41:20.835761Z","shell.execute_reply":"2025-05-25T14:41:20.842499Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nimage_size = 224\n\ntransform = transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.Resize((image_size, image_size)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.5]*3, [0.5]*3)\n])\n\ntrain_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42)\n\ntrain_dataset = SoilDataset(train_df, train_image_dir, transform=transform)\nval_dataset = SoilDataset(val_df, train_image_dir, transform=transform)\ntest_dataset = SoilDataset(test_df, test_image_dir, transform=transform, include_label=False)\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n","metadata":{"id":"jFGcAz3sp-Ju","trusted":true,"execution":{"iopub.status.busy":"2025-05-25T14:41:25.599131Z","iopub.execute_input":"2025-05-25T14:41:25.599427Z","iopub.status.idle":"2025-05-25T14:41:25.608756Z","shell.execute_reply.started":"2025-05-25T14:41:25.599401Z","shell.execute_reply":"2025-05-25T14:41:25.608012Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"from torchvision import models\nimport torch.nn as nn\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Load pretrained ResNet18, remove final layer\nresnet = models.resnet18(pretrained=True)\nresnet.fc = nn.Identity()\nresnet = resnet.to(device)\nresnet.eval()\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cYf45Xvgp_0Y","outputId":"65da84b5-420d-4a92-fdec-c6521d94f1b3","trusted":true,"execution":{"iopub.status.busy":"2025-05-25T14:41:29.463376Z","iopub.execute_input":"2025-05-25T14:41:29.464003Z","iopub.status.idle":"2025-05-25T14:41:29.678779Z","shell.execute_reply.started":"2025-05-25T14:41:29.463978Z","shell.execute_reply":"2025-05-25T14:41:29.677763Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n","output_type":"stream"},{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"ResNet(\n  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu): ReLU(inplace=True)\n  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  (layer1): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer2): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer3): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer4): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n  (fc): Identity()\n)"},"metadata":{}}],"execution_count":27},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.svm import OneClassSVM\n\n# Feature extraction function\ndef extract_features(loader):\n    features_list = []\n    labels_list = []\n    with torch.no_grad():\n        for images, labels in loader:\n            images = images.to(device)\n            features = resnet(images).cpu().numpy()\n            features_list.append(features)\n            labels_list.append(labels)\n    return np.vstack(features_list), np.hstack(labels_list)\n\ntrain_features, _ = extract_features(train_loader)\n\n# Train One-Class SVM: soil = normal, everything else = outlier\nsvm_model = OneClassSVM(kernel='rbf', nu=0.1, gamma='scale')\nsvm_model.fit(train_features)\nprint(\"✅ One-Class SVM trained.\")\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5TLY70o4qBsN","outputId":"c17f7644-5406-4df8-c683-27253f93515e","trusted":true,"execution":{"iopub.status.busy":"2025-05-25T14:41:40.259183Z","iopub.execute_input":"2025-05-25T14:41:40.259710Z","iopub.status.idle":"2025-05-25T14:42:34.385982Z","shell.execute_reply.started":"2025-05-25T14:41:40.259685Z","shell.execute_reply":"2025-05-25T14:42:34.385121Z"}},"outputs":[{"name":"stdout","text":"✅ One-Class SVM trained.\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"test_predictions = []\n\nwith torch.no_grad():\n    for images, image_ids in DataLoader(test_dataset, batch_size=32, shuffle=False):\n        images = images.to(device)\n        features = resnet(images).cpu().numpy()\n        preds = svm_model.predict(features)  # 1 = in-class (Soil), -1 = outlier (Non-Soil)\n\n        for img_id, pred in zip(image_ids, preds):\n            label = '1' if pred == 1 else '0'\n            test_predictions.append((img_id, label))\n\n# Save predictions\nsubmission = pd.DataFrame(test_predictions, columns=['image_id', 'label'])\nsubmission.to_csv(\"submission.csv\", index=False)\nprint(\"✅ Predictions saved to submission.csv\")\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OI3aOZlVqEil","outputId":"b4ec48ee-0757-44ff-e1cf-dda5f91f9aa3","trusted":true,"execution":{"iopub.status.busy":"2025-05-25T14:42:41.448075Z","iopub.execute_input":"2025-05-25T14:42:41.448949Z","iopub.status.idle":"2025-05-25T14:43:24.895717Z","shell.execute_reply.started":"2025-05-25T14:42:41.448917Z","shell.execute_reply":"2025-05-25T14:43:24.894892Z"}},"outputs":[{"name":"stdout","text":"✅ Predictions saved to submission.csv\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}